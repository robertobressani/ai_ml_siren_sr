{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SIREN_SR.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "n0t41qCjXFKP",
        "vqtDflowR89D",
        "re6No741zN_Z",
        "qqhldt7jTw2v",
        "e-slLOQGT_4H",
        "GmYKPaK7UBk5",
        "9rInYupXV2DH",
        "NzHNCzN9dGVD",
        "mUnY0rb_eKyU",
        "vK0jfFhKeX8j",
        "1rTBJ40Cff8O",
        "4Xmo2wlJh_wY",
        "2cJ3a8dsh_wZ",
        "DcVoYf_sjF3A"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JM2ts3ZrqmX"
      },
      "source": [
        "# Sinusoidal Representation Network for Single Image Super Resolution\r\n",
        "This is the implementation of the study conducted in https://github.com/robertobressani/ai_ml_siren_sr\r\n",
        "\r\n",
        "The following code (except from [Using ESDR](##using-edsr) which uses TPU) must be run by selecting GPU as hardware accelerator (Runtime -> change runtime type)\r\n",
        "\r\n",
        "The first 3 blocks should be run once before running every experiment\r\n",
        "\r\n",
        "NOTE: some blocks can have problems with output logging. `train` accept a `with_log` param to be set to `False` to avoid this problem (all the output is however reported under `/plots/*` folder)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhIIKcKTqOOQ"
      },
      "source": [
        "!rm -r ai_ml_siren_sr\r\n",
        "!git clone -b main https://github.com/robertobressani/ai_ml_siren_sr\r\n",
        "!pip3 install 'torch'\r\n",
        "!pip3 install 'torchvision'\r\n",
        "!pip3 install 'kornia'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CosjUZP5qRli"
      },
      "source": [
        "BASE_DIR='/content/ai_ml_siren_sr'\r\n",
        "DEVICE= 'cuda'\r\n",
        "\r\n",
        "import sys\r\n",
        "sys.path.insert(1, BASE_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8aSbpvTqvx0"
      },
      "source": [
        "import math\r\n",
        "import torch\r\n",
        "from torch import nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import os\r\n",
        "import PIL\r\n",
        "import matplotlib\r\n",
        "from torchvision.transforms import Resize\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy\r\n",
        "\r\n",
        "\r\n",
        "from utils import data_utils, math_utils, summary_utils\r\n",
        "from core.trainer import Trainer\r\n",
        "from core.network import NetworkParams, NetworkDimensions\r\n",
        "from datasets.AudioSignalDataset import AudioSignal\r\n",
        "from datasets.ImageFittingDataset import ImageFitting\r\n",
        "from datasets.PoissonImageDataset import PoissonImageDataset\r\n",
        "\r\n",
        "from layers.relulayer import ReLuLayer\r\n",
        "from layers.sinelayer import SineLayer\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTWTDKJ1tAS1"
      },
      "source": [
        "# Audio signal fitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1XyFZnfqzdX"
      },
      "source": [
        "playlist = [\"gt_bach\", \"gt_counting\", \"gt_movie\"]\r\n",
        "\r\n",
        "# defining experiments\r\n",
        "nets_params = [\r\n",
        "               NetworkParams(layer_class= ReLuLayer, description='relu'),\r\n",
        "               NetworkParams(description='siren'),\r\n",
        "               NetworkParams(description='siren_omega_r'), \r\n",
        "               NetworkParams(description='siren_omega_r_fft'), \r\n",
        "               ]\r\n",
        "\r\n",
        "iterations= 500\r\n",
        "\r\n",
        "\r\n",
        "def get_first_omega_0(description, omega_r):\r\n",
        "  first_omega_0 = 30\r\n",
        "  if description.find(\"siren\") >= 0:\r\n",
        "    if description.find(\"omega_r\") >= 0:\r\n",
        "      first_omega_0 = omega_r\r\n",
        "    else:\r\n",
        "      first_omega_0 = 3000\r\n",
        "  \r\n",
        "  return first_omega_0\r\n",
        "\r\n",
        "\r\n",
        "def get_hidden_layers(num_samples, channels):\r\n",
        "  return min(5, 1 + int((0.6 * num_samples * channels) / 256 ** 2))\r\n",
        "\r\n",
        "\r\n",
        "for net_params in nets_params:\r\n",
        "\r\n",
        "    # define a trainer for each params\r\n",
        "    trainer = Trainer(net_params, device=DEVICE)\r\n",
        "\r\n",
        "    for name in playlist:\r\n",
        "        # load dataset\r\n",
        "        dataset = AudioSignal(f\"{BASE_DIR}/data/{name}.wav\", name=name)\r\n",
        "\r\n",
        "        # get dynamically hidden layers\r\n",
        "        hidden_layers = get_hidden_layers(dataset.get_num_samples(), dataset.channels)\r\n",
        "\r\n",
        "        # get omega_0 for the first layer\r\n",
        "        first_omega_0 = get_first_omega_0(net_params.description, dataset.rate)\r\n",
        "\r\n",
        "        # prepare network dimensions\r\n",
        "        dims = NetworkDimensions(1, dataset.channels, hidden_layers=hidden_layers, first_omega_0=first_omega_0)\r\n",
        "\r\n",
        "        # define the loss function to use\r\n",
        "        combining_losses = net_params.description.find(\"fft\") >= 0 \r\n",
        "        loss_fn = (lambda gt, model_output : \\\r\n",
        "                   data_utils.get_fft_mse(gt, model_output, 50)) if combining_losses \\\r\n",
        "                   else data_utils.get_mse\r\n",
        "\r\n",
        "        # train\r\n",
        "        print(f\"\\nTrain with:\\nhidden_layers={hidden_layers}\\nfirts omega={first_omega_0}\\ncombining losses={combining_losses}\\n\")\r\n",
        "        trainer.train(dims, [dataset], [iterations], summary_fn=summary_utils.audio_summary, lr=1e-4, loss_fn=loss_fn,\r\n",
        "                      patience=min(100, 10 * hidden_layers))\r\n",
        "        \r\n",
        "        # test\r\n",
        "        trainer.test(dataset, validate_fn=summary_utils.audio_validate)\r\n",
        "\r\n",
        "    mean, std = trainer.statistics()\r\n",
        "    print(f\"\\nMSE Mean {mean[0]} and std {std[0]} for {net_params.description}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uNSD8HqtiiH"
      },
      "source": [
        "# Image fitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bHRLQZ2tkn9"
      },
      "source": [
        "images = data_utils.get_div2k_images()\r\n",
        "\r\n",
        "# defining experiments\r\n",
        "nets_params = [\r\n",
        "               NetworkParams(description='ImageFitting with SIREN'), \r\n",
        "               NetworkParams(layer_class= ReLuLayer, description='ImageFitting using ReLU activation'),\r\n",
        "               NetworkParams(description='ImageFitting with SIREN + custom omega_0 ')\r\n",
        "               ]\r\n",
        "iterations= 5000\r\n",
        "hidden_layers = 2\r\n",
        "\r\n",
        "for net_params in nets_params:\r\n",
        "  # defining trainer \r\n",
        "  trainer = Trainer(net_params, device=DEVICE)\r\n",
        "  for image_name in images: \r\n",
        "      # training and test for each image        \r\n",
        "      image = data_utils.get_div2k_image(image_name, dir=BASE_DIR+\"/\", resolution= 'low')\r\n",
        "\r\n",
        "      # loading the dataset \r\n",
        "      dataset = ImageFitting(data_image=image, name= image_name, normalized=True)\r\n",
        "\r\n",
        "      #computing omega_0* basing on laplacian\r\n",
        "      lapl = PoissonImageDataset(data_image = image, name= image_name, fit_laplacian = True)      \r\n",
        "      first_omega_0 = torch.std(lapl.gt).detach()*250 if net_params.description.find(\"omega\")>=0 else 30\r\n",
        "\r\n",
        "      #dimensioning the network\r\n",
        "      dims = NetworkDimensions(2, dataset.channels, hidden_layers=hidden_layers, first_omega_0 = first_omega_0)\r\n",
        "\r\n",
        "      # defining the experiment on the basis of the experiment\r\n",
        "      lr = 1e-4 if net_params.layer_class == ReLuLayer else 5e-4\r\n",
        "\r\n",
        "      #training the network\r\n",
        "      trainer.train(dims, [dataset], [iterations], summary_fn=summary_utils.image_fitting_summary, \r\n",
        "                    loss_fn=data_utils.get_mse, lr=lr,\r\n",
        "                    patience = max(100, 10*hidden_layers))\r\n",
        "            \r\n",
        "      # testing the representation obtained             \r\n",
        "      trainer.test(dataset, validate_fn=summary_utils.image_fitting_validate)\r\n",
        "\r\n",
        "  # getting results for each experiment\r\n",
        "  mean, std = trainer.statistics(compute_PSNR=True)\r\n",
        "  print(f\"PSNR Mean {mean[0]} dB and std {std[0]} dB for {net_params.description}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjEhlvAz1B-p"
      },
      "source": [
        "# Solving Poisson Equation\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fj8xtJGjtJJO"
      },
      "source": [
        "\r\n",
        "## Training on gradient"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6niItrli1Bo3"
      },
      "source": [
        "images = data_utils.get_div2k_images()\r\n",
        "\r\n",
        "# defining experiments to perform\r\n",
        "nets_params = [\r\n",
        "               NetworkParams(description='Poisson trained on grad with SIREN'), \r\n",
        "               NetworkParams(description='Poisson trained on grad with SIREN with numerical methods'), \r\n",
        "               NetworkParams(layer_class= ReLuLayer, description='Poisson trained on grad using ReLU activation with numerical methods')\r\n",
        "               ]\r\n",
        "iterations= 5000\r\n",
        "hidden_layers = 2\r\n",
        "\r\n",
        "desc= [\"Image\", \"Gradient\", \"Laplacian\"]\r\n",
        "\r\n",
        "for net_params in nets_params:\r\n",
        "  # defining trainer for every experiment\r\n",
        "  trainer = Trainer(net_params, device=DEVICE)\r\n",
        "  for image_name in images:         \r\n",
        "\r\n",
        "      # defining dataset\r\n",
        "      image = data_utils.get_div2k_image(image_name, dir=BASE_DIR+\"/\", resolution= 'low')\r\n",
        "      dataset = PoissonImageDataset(data_image=image, name=image_name,\r\n",
        "                              fit_laplacian=False, normalized=True)   \r\n",
        "      \r\n",
        "      # computing omega_0* basing on laplacian\r\n",
        "      lapl = PoissonImageDataset(data_image = image, name= image_name, fit_laplacian = True)\r\n",
        "      first_omega_0 = torch.std(lapl.gt).detach()*250 if net_params.layer_class == SineLayer else 30\r\n",
        "    \r\n",
        "      # computing transformation on which to compute the loss\r\n",
        "      manipulation = data_utils.get_manipulator(\"grad_num\") if net_params.description.find(\"numerical\")>=0 \\\r\n",
        "          else data_utils.get_manipulator(\"grad\", .1)\r\n",
        "      \r\n",
        "      # defining network dimensions\r\n",
        "      dims = NetworkDimensions(2, dataset.channels, hidden_layers=hidden_layers, first_omega_0=first_omega_0)\r\n",
        "\r\n",
        "      # defining learning rate for the experiment\r\n",
        "      lr= 5e-4 if net_params.layer_class == SineLayer and net_params.description.find(\"numerical\")>0 else 1e-4\r\n",
        "\r\n",
        "      # training the net\r\n",
        "      trainer.train(dims, [dataset], [iterations], summary_fn=summary_utils.poisson_image_summary, \r\n",
        "                    loss_fn=data_utils.get_mse, lr=lr, output_manipulation=manipulation,\r\n",
        "                    patience = max(100, 10*hidden_layers)\r\n",
        "                    )\r\n",
        "      # defining validation function\r\n",
        "      validation =summary_utils.poisson_image_validate  if net_params.description.find(\"numerical\")>=0  else \\\r\n",
        "          lambda model_output, coords, dataset, layer_folder :\\\r\n",
        "           summary_utils.poisson_image_validate( model_output, coords, dataset, layer_folder, numerical=False, \r\n",
        "                                                lapl_factor = 0.05, grad_factor=2.5)\r\n",
        "\r\n",
        "      # testing results\r\n",
        "      trainer.test(dataset, validate_fn=validation)\r\n",
        "\r\n",
        "  # reporting results\r\n",
        "  mean, std = trainer.statistics(compute_PSNR=True)\r\n",
        "  print(f\"{net_params.description}:\")\r\n",
        "  for i in range(len(mean)):\r\n",
        "    print(f\"  {desc[i]}\\t (mean, std): {mean[i]},{std[i]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvwmrERS4GmP"
      },
      "source": [
        "## Training on laplacian"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5ezHTCm4JFO"
      },
      "source": [
        "images = data_utils.get_div2k_images()\r\n",
        "\r\n",
        "#defining experiments\r\n",
        "nets_params = [\r\n",
        "    NetworkParams(description='Poisson trained on laplacian with SIREN with numerical methods'), \r\n",
        "    NetworkParams(layer_class= ReLuLayer, description='Poisson trained on laplacian using ReLU activation with numerical methods')\r\n",
        "               ]\r\n",
        "iterations= 5000\r\n",
        "hidden_layers = 2\r\n",
        "\r\n",
        "desc= [\"Image\", \"Gradient\", \"Laplacian\"]\r\n",
        "\r\n",
        "for net_params in nets_params:\r\n",
        "  # defining trainer for every experiment\r\n",
        "  trainer = Trainer(net_params, device=DEVICE)\r\n",
        "  for image_name in images: \r\n",
        "       # defining dataset        \r\n",
        "      image = data_utils.get_div2k_image(image_name, dir=BASE_DIR+\"/\", resolution= 'low')\r\n",
        "      dataset = PoissonImageDataset(data_image=image, name=image_name,\r\n",
        "                              fit_laplacian=True, normalized=True)   \r\n",
        "      \r\n",
        "       # computing omega_0* \r\n",
        "      first_omega_0 = torch.std(dataset.gt).detach()*250 if net_params.layer_class == SineLayer else 30\r\n",
        "    \r\n",
        "      # computing transformation on which to compute the loss\r\n",
        "      manipulation = data_utils.get_manipulator(\"lapl_num\") if net_params.description.find(\"numerical\")>=0 \\\r\n",
        "          else data_utils.get_manipulator(\"lapl\", 0.05)\r\n",
        "      \r\n",
        "      # defining network dimensions\r\n",
        "      dims = NetworkDimensions(2, dataset.channels, hidden_layers=hidden_layers, first_omega_0=first_omega_0)\r\n",
        "\r\n",
        "      # defining learning rate for the experiment\r\n",
        "      lr= 5e-4 if net_params.layer_class == SineLayer else 1e-4\r\n",
        "\r\n",
        "      # training the net\r\n",
        "      trainer.train(dims, [dataset], [iterations], summary_fn=summary_utils.poisson_image_summary, \r\n",
        "                    loss_fn=data_utils.get_mse, lr=lr, output_manipulation=manipulation,\r\n",
        "                    patience = max(100, 10*hidden_layers)\r\n",
        "                    )\r\n",
        "      # defining validation function\r\n",
        "      validation =summary_utils.poisson_image_validate  if net_params.description.find(\"numerical\")>=0  else \\\r\n",
        "          lambda model_output, coords, dataset, layer_folder :\\\r\n",
        "           summary_utils.poisson_image_validate( model_output, coords, dataset, layer_folder, numerical=False,\r\n",
        "                                                lapl_factor = 0.05, grad_factor=2.5)\r\n",
        "\r\n",
        "      # testing results\r\n",
        "      trainer.test(dataset, validate_fn=validation)\r\n",
        "\r\n",
        "  # reporting results\r\n",
        "  mean, std = trainer.statistics(compute_PSNR=True)\r\n",
        "  print(f\"{net_params.description}:\")\r\n",
        "  for i in range(len(mean)):\r\n",
        "    print(f\"  {desc[i]}\\t (mean, std): {mean[i]},{std[i]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSUJY7sg5fNP"
      },
      "source": [
        "# Exploiting super resolution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dna9MqiOzPD_"
      },
      "source": [
        "## Using bicubic method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfBuKEso5lpI"
      },
      "source": [
        "UPSCALING = 4\r\n",
        "\r\n",
        "images = data_utils.get_div2k_images()\r\n",
        "\r\n",
        "os.makedirs(f\"./plots/bicubic/\", exist_ok=True)\r\n",
        "\r\n",
        "results =[]\r\n",
        "\r\n",
        "for image_name in images:\r\n",
        "    # getting images\r\n",
        "    image_hr = data_utils.get_image_tensor(data_utils.get_div2k_image(image_name, dir=BASE_DIR+\"/\"), down_scale=1)\r\n",
        "    image_lr = data_utils.get_image_tensor(data_utils.get_div2k_image(image_name, dir=BASE_DIR+\"/\", resolution='low'), down_scale=1)\r\n",
        "    channels, height, width = image_hr.shape\r\n",
        "    \r\n",
        "    # upsampling using bicubic\r\n",
        "    super_resolution = Resize([int(height), int(width)], interpolation=PIL.Image.BICUBIC)\r\n",
        "    output = super_resolution(image_lr)\r\n",
        "\r\n",
        "    image = data_utils.to_hwc(image_hr)\r\n",
        "    output = data_utils.to_hwc(torch.clamp(output, min=0, max=1))\r\n",
        "\r\n",
        "    # measuring the results\r\n",
        "    mse = data_utils.get_mse(image, output)\r\n",
        "    PSNR = math_utils.PSNR(mse)\r\n",
        "\r\n",
        "    if channels == 1:\r\n",
        "        image = image.view(height, width)\r\n",
        "        output = output.view(height, width)\r\n",
        "\r\n",
        "    # plotting and saving results\r\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(18, 7))\r\n",
        "    plt.suptitle(\"Bicubic Super Resolution\", fontsize=15)\r\n",
        "    axes[0].imshow(image.cpu().detach().numpy())\r\n",
        "    axes[0].set_title(\"Ground truth\")\r\n",
        "    axes[1].imshow(output.cpu().detach().numpy())\r\n",
        "    axes[1].set_title(f\"Reconstruction x{UPSCALING}\")\r\n",
        "    plt.savefig(f\"./plots/bicubic/{image_name}_x4.png\")\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "    matplotlib.image.imsave(f\"./plots/bicubic/{image_name}_x4_reconstruction.png\", output.detach().numpy())\r\n",
        "\r\n",
        "\r\n",
        "    print(image_name,\"\\t mse: \", mse, \" PSNR: \", PSNR)\r\n",
        "    results.append(PSNR)\r\n",
        "\r\n",
        "print(f\"Bicubic SNR (mean,std): {numpy.mean(results)}, {numpy.std(results)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m69NKIcz7gqx"
      },
      "source": [
        "## Using EDSR\r\n",
        "\r\n",
        "Testing results of EDSR on our dataset.\r\n",
        "\r\n",
        "**NOTE: Pay attention that this code must be run using TPU and not GPU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVNTrXIe7pp_"
      },
      "source": [
        "! git clone https://github.com/krasserm/super-resolution\r\n",
        "! mv super-resolution EDSR\r\n",
        "\r\n",
        "import sys\r\n",
        "\r\n",
        "sys.path.insert(1, '/content/EDSR')\r\n",
        "\r\n",
        "! wget https://martin-krasser.de/sisr/weights-edsr-16-x4.tar.gz\r\n",
        "\r\n",
        "! tar xvfz weights-edsr-16-x4.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3649Bznj7lGg"
      },
      "source": [
        "from model import resolve_single\r\n",
        "from model.common import psnr\r\n",
        "from model.edsr import edsr\r\n",
        "import tensorflow as tf\r\n",
        "import statistics\r\n",
        "\r\n",
        "from EDSR.utils import load_image, plot_sample\r\n",
        "\r\n",
        "model = edsr(scale=4, num_res_blocks=16)\r\n",
        "model.load_weights('weights/edsr-16-x4/weights.h5')\r\n",
        "\r\n",
        "images = data_utils.get_div2k_images()\r\n",
        "images_hr = [load_image(f\"{BASE_DIR}/data/images/resized/{image}.png\") for image in images]\r\n",
        "images_lr = [load_image(f\"{BASE_DIR}/data/images/resized/{image}x4.png\")  for image in images]\r\n",
        "p = []\r\n",
        "for i in range(len(images)):\r\n",
        "  lr = images_lr[i]\r\n",
        "  sr = resolve_single(model, lr)\r\n",
        "\r\n",
        "  gt = images_hr[i]\r\n",
        "  ps = float(tf.get_static_value(psnr(gt, sr)))\r\n",
        "  p.append(ps)\r\n",
        "  print(ps)\r\n",
        "  \r\n",
        "  plot_sample(lr, sr)\r\n",
        "\r\n",
        "print(\"PSNR (mean, std):\", statistics.mean(p), \",\", statistics.stdev(p))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZkB1pGK9tGr"
      },
      "source": [
        "## Using SIREN\r\n",
        "Results are reported also under `plots/image_super_resolution/Super Resolution */results` to appreciate better differiencies between hr and ground_truth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fK1CklDc4LHZ"
      },
      "source": [
        "### Derivation of $\\omega_{HR}$\r\n",
        "This is a run on training image. Same results are obtained with the whole DIV2K validation dataset (to avoid to overload the network for this experiment it has not been uploaded on the repository)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cY1fMLPp0QeQ"
      },
      "source": [
        "images = data_utils.get_div2k_images()\r\n",
        "res =[]\r\n",
        "\r\n",
        "for image_name in images:\r\n",
        "    \r\n",
        "    # getting omega_0* for low resolution images upsampled to HR with bicubic\r\n",
        "    image = data_utils.get_div2k_image(image_name, dir='ai_ml_siren_sr/', resolution= 'low')\r\n",
        "    lapl = PoissonImageDataset(data_image = image, name= image_name, fit_laplacian = True, up_scale=4)\r\n",
        "    \r\n",
        "    # getting omega_0 of testing HR images\r\n",
        "    image_hr =data_utils.get_div2k_image(image_name, dir='ai_ml_siren_sr/', resolution= 'high')\r\n",
        "    lapl_hr =  PoissonImageDataset(data_image = image_hr, name= image_name, fit_laplacian = True)\r\n",
        "    \r\n",
        "    # computing their relation\r\n",
        "    res.append( torch.std(lapl.gt).detach()/torch.std(lapl_hr.gt).detach())\r\n",
        "      \r\n",
        "print(numpy.mean(res), numpy.std(res))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OG7Soqxz5AzZ"
      },
      "source": [
        "### Basic SIREN training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7whNLoEy9wef"
      },
      "source": [
        "images = data_utils.get_div2k_images()\r\n",
        "\r\n",
        "resolutions = [\r\n",
        "        {\"down\": 1, \"up\": 1},\r\n",
        "]\r\n",
        "iterations = [5000]\r\n",
        "net_params = NetworkParams(description=\"Super Resolution basic\")\r\n",
        "\r\n",
        "hidden_layers=2\r\n",
        "\r\n",
        "\r\n",
        "trainer = Trainer(net_params, device='cuda')\r\n",
        "for image_name in images:\r\n",
        "    # getting image\r\n",
        "    image = data_utils.get_div2k_image(image_name, dir='ai_ml_siren_sr/', resolution= 'low')\r\n",
        "\r\n",
        "    # getting list of dataset (one element in basic case)\r\n",
        "    datasets = list(map(lambda item: ImageFitting(data_image=image, name=image_name, normalized=True,\r\n",
        "                                                    down_scale=item[\"down\"],\r\n",
        "                                                    up_scale=item[\"up\"]     \r\n",
        "                                                    ), resolutions))\r\n",
        "    \r\n",
        "    # computing omega_HR\r\n",
        "    lapl = PoissonImageDataset(data_image = image, name= image_name, fit_laplacian = True, up_scale=4) \r\n",
        "    trainer.params.first_omega_0 =torch.std(lapl.gt).detach()*250/0.15\r\n",
        "    \r\n",
        "    dims = NetworkDimensions(2, datasets[0].channels, hidden_layers=hidden_layers, hidden_features=256)\r\n",
        "\r\n",
        "    # training the network\r\n",
        "    trainer.train(dims,datasets, iterations, summary_fn=summary_utils.image_super_resolution_summary, \r\n",
        "                  loss_fn=data_utils.get_mse, lr=5e-4,\r\n",
        "                  regularization=5e-6, \r\n",
        "                  output_manipulation = data_utils.get_manipulator('grad_num', .10),\r\n",
        "                  patience = max(100, 10*hidden_layers)\r\n",
        "                  )\r\n",
        "    # getting HR image\r\n",
        "    image = data_utils.get_div2k_image(image_name, dir='ai_ml_siren_sr/')\r\n",
        "    dataset_hr =  ImageFitting(data_image=image, name= image_name, normalized=True)\r\n",
        "\r\n",
        "    # testing\r\n",
        "    trainer.test(dataset_hr, validate_fn=summary_utils.image_super_resolution_validate)\r\n",
        "\r\n",
        "# reporting results\r\n",
        "mean, std = trainer.statistics(compute_PSNR=True)\r\n",
        "for i in range (len(mean)):\r\n",
        "    print(f\"PSNR Mean {mean[i]} dB and std {std[i]} dB for Basic SIREN SR\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNmLQe_H6d0_"
      },
      "source": [
        "### SIREN training trick 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFIuFjJP6d1A"
      },
      "source": [
        "images = data_utils.get_div2k_images()\r\n",
        "\r\n",
        "resolutions = [\r\n",
        "        {\"down\": 4, \"up\": 4},\r\n",
        "        {\"down\": 3, \"up\": 3},\r\n",
        "        {\"down\": 2, \"up\": 2},\r\n",
        "        {\"down\": 1.5, \"up\": 1.5},\r\n",
        "        {\"down\": 1, \"up\": 1},\r\n",
        "        {\"down\": 1, \"up\": 1.5},\r\n",
        "        {\"down\": 1, \"up\": 2},\r\n",
        "]\r\n",
        "iterations = [500,500,500,500,1000,1000,1000]\r\n",
        "net_params = NetworkParams(description=\"Super Resolution trick1\")\r\n",
        "\r\n",
        "hidden_layers=2\r\n",
        "\r\n",
        "\r\n",
        "trainer = Trainer(net_params, device='cuda')\r\n",
        "for image_name in images:\r\n",
        "    # getting image\r\n",
        "    image = data_utils.get_div2k_image(image_name, dir='ai_ml_siren_sr/', resolution= 'low')\r\n",
        "\r\n",
        "    # getting list of dataset (one element in basic case)\r\n",
        "    datasets = list(map(lambda item: ImageFitting(data_image=image, name=image_name, normalized=True,\r\n",
        "                                                    down_scale=item[\"down\"],\r\n",
        "                                                    up_scale=item[\"up\"]     \r\n",
        "                                                    ), resolutions))\r\n",
        "    \r\n",
        "    # computing omega_HR\r\n",
        "    lapl = PoissonImageDataset(data_image = image, name= image_name, fit_laplacian = True, up_scale=4) \r\n",
        "    trainer.params.first_omega_0 =torch.std(lapl.gt).detach()*250/0.15\r\n",
        "    \r\n",
        "    dims = NetworkDimensions(2, datasets[0].channels, hidden_layers=hidden_layers, hidden_features=256)\r\n",
        "\r\n",
        "    # training the network\r\n",
        "    trainer.train(dims,datasets, iterations, summary_fn=summary_utils.image_super_resolution_summary, \r\n",
        "                  loss_fn=data_utils.get_mse, lr=5e-4,\r\n",
        "                  regularization=5e-6, \r\n",
        "                  output_manipulation = data_utils.get_manipulator('grad_num', .10),\r\n",
        "                  patience = max(100, 10*hidden_layers)\r\n",
        "                  )\r\n",
        "    # getting HR image\r\n",
        "    image = data_utils.get_div2k_image(image_name, dir='ai_ml_siren_sr/')\r\n",
        "    dataset_hr =  ImageFitting(data_image=image, name= image_name, normalized=True)\r\n",
        "\r\n",
        "    # testing\r\n",
        "    trainer.test(dataset_hr, validate_fn=summary_utils.image_super_resolution_validate)\r\n",
        "\r\n",
        "# reporting results\r\n",
        "mean, std = trainer.statistics(compute_PSNR=True)\r\n",
        "for i in range (len(mean)):\r\n",
        "    print(f\"PSNR Mean {mean[i]} dB and std {std[i]} dB for SIREN SR trick1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPWAR9TS7QIF"
      },
      "source": [
        "### SIREN training trick 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TU6r0hjb7QIG"
      },
      "source": [
        "images = data_utils.get_div2k_images()\r\n",
        "\r\n",
        "resolutions = [\r\n",
        "        {\"down\": 1, \"up\": 1},\r\n",
        "]\r\n",
        "iterations = [1000]\r\n",
        "\r\n",
        "for i in range(1,21):\r\n",
        "  # defining all small training steps\r\n",
        "  resolutions.append({\"down\":1, \"up\":1+(0.1*i)})\r\n",
        "  iterations.append(200)\r\n",
        "\r\n",
        "net_params = NetworkParams(description=\"Super Resolution trick2\")\r\n",
        "\r\n",
        "hidden_layers=2\r\n",
        "\r\n",
        "\r\n",
        "trainer = Trainer(net_params, device='cuda')\r\n",
        "for image_name in images:\r\n",
        "    # getting image\r\n",
        "    image = data_utils.get_div2k_image(image_name, dir='ai_ml_siren_sr/', resolution= 'low')\r\n",
        "\r\n",
        "    # getting list of dataset (one element in basic case)\r\n",
        "    datasets = list(map(lambda item: ImageFitting(data_image=image, name=image_name, normalized=True,\r\n",
        "                                                    down_scale=item[\"down\"],\r\n",
        "                                                    up_scale=item[\"up\"]     \r\n",
        "                                                    ), resolutions))\r\n",
        "    \r\n",
        "    # computing omega_HR\r\n",
        "    lapl = PoissonImageDataset(data_image = image, name= image_name, fit_laplacian = True, up_scale=4) \r\n",
        "    trainer.params.first_omega_0 =torch.std(lapl.gt).detach()*250/0.15\r\n",
        "    \r\n",
        "    dims = NetworkDimensions(2, datasets[0].channels, hidden_layers=hidden_layers, hidden_features=256)\r\n",
        "\r\n",
        "    # training the network\r\n",
        "    trainer.train(dims,datasets, iterations, summary_fn=summary_utils.image_super_resolution_summary, \r\n",
        "                  loss_fn=data_utils.get_mse, lr=5e-4,\r\n",
        "                  regularization=5e-6, \r\n",
        "                  output_manipulation = data_utils.get_manipulator('grad_num', .10),\r\n",
        "                  patience = max(100, 10*hidden_layers)\r\n",
        "                  )\r\n",
        "    # getting HR image\r\n",
        "    image = data_utils.get_div2k_image(image_name, dir='ai_ml_siren_sr/')\r\n",
        "    dataset_hr =  ImageFitting(data_image=image, name= image_name, normalized=True)\r\n",
        "\r\n",
        "    # testing\r\n",
        "    trainer.test(dataset_hr, validate_fn=summary_utils.image_super_resolution_validate)\r\n",
        "\r\n",
        "# reporting results\r\n",
        "mean, std = trainer.statistics(compute_PSNR=True)\r\n",
        "for i in range (len(mean)):\r\n",
        "    print(f\"PSNR Mean {mean[i]} dB and std {std[i]} dB for SIREN SR trick2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0t41qCjXFKP"
      },
      "source": [
        "# Ablation studies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frYDLgxjTCfF"
      },
      "source": [
        "# run this code before ablation study execution\r\n",
        "from core.network import Network"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqtDflowR89D"
      },
      "source": [
        "## Baseline for activation distributions\r\n",
        "\r\n",
        "Analyzing activations and spectrum under Sitzmann's initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYEMp8oqR1eo"
      },
      "source": [
        "  dims = NetworkDimensions(in_features=1, out_features=1, hidden_layers=2, hidden_features=2048, first_omega_0=30)\r\n",
        "  params = NetworkParams(outermost_linear=True)\r\n",
        "  model = Network(params=params, dimensions=dims)\r\n",
        "\r\n",
        "  input_signal = torch.linspace(-1, 1, 65536//4).view(1, -1, 1)\r\n",
        "  activations = model.forward_with_activations(input_signal, retain_grad=True)\r\n",
        "\r\n",
        "  output = activations[next(reversed(activations))]\r\n",
        "\r\n",
        "  output.mean().backward()\r\n",
        "\r\n",
        "  data_utils.plot_all_activations_and_grads(activations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "re6No741zN_Z"
      },
      "source": [
        "## First layer $\\omega_0$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqhldt7jTw2v"
      },
      "source": [
        "### $\\omega_0 = 1$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbQMSoN5RqfY"
      },
      "source": [
        "omega_0 = 1\n",
        "print(f\"Network with omega 0={omega_0}\")\n",
        "dims = NetworkDimensions(in_features=1, out_features=1, hidden_layers=2, hidden_features=2048, first_omega_0=omega_0)\n",
        "params = NetworkParams(outermost_linear=True)\n",
        "model = Network(params=params, dimensions=dims)\n",
        "\n",
        "input_signal = torch.linspace(-1, 1, 65536//4).view(1, -1, 1)\n",
        "# generating the output and activations for a uniform input\n",
        "activations = model.forward_with_activations(input_signal, retain_grad=True)\n",
        "output = activations[next(reversed(activations))]\n",
        "output.mean().backward()\n",
        "\n",
        "# plot activations at every layer\n",
        "data_utils.plot_all_activations_and_grads(activations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-slLOQGT_4H"
      },
      "source": [
        "### $\\omega_0 = 30$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRrRp8vvRecW"
      },
      "source": [
        "omega_0 = 30\n",
        "print(f\"Network with omega 0={omega_0}\")\n",
        "dims = NetworkDimensions(in_features=1, out_features=1, hidden_layers=2, hidden_features=2048, first_omega_0=omega_0)\n",
        "params = NetworkParams(outermost_linear=True)\n",
        "model = Network(params=params, dimensions=dims)\n",
        "\n",
        "input_signal = torch.linspace(-1, 1, 65536//4).view(1, -1, 1)\n",
        "# generating the output and activations for a uniform input\n",
        "activations = model.forward_with_activations(input_signal, retain_grad=True)\n",
        "output = activations[next(reversed(activations))]\n",
        "output.mean().backward()\n",
        "\n",
        "# plot activations at every layer\n",
        "data_utils.plot_all_activations_and_grads(activations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmYKPaK7UBk5"
      },
      "source": [
        "### $\\omega_0 = 1000$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzvNZ1QLT7-g"
      },
      "source": [
        "omega_0 = 1000\n",
        "print(f\"Network with omega 0={omega_0}\")\n",
        "dims = NetworkDimensions(in_features=1, out_features=1, hidden_layers=2, hidden_features=2048, first_omega_0=omega_0)\n",
        "params = NetworkParams(outermost_linear=True)\n",
        "model = Network(params=params, dimensions=dims)\n",
        "\n",
        "input_signal = torch.linspace(-1, 1, 65536//4).view(1, -1, 1)\n",
        "# generating the output and activations for a uniform input\n",
        "activations = model.forward_with_activations(input_signal, retain_grad=True)\n",
        "output = activations[next(reversed(activations))]\n",
        "output.mean().backward()\n",
        "\n",
        "# plot activations at every layer\n",
        "data_utils.plot_all_activations_and_grads(activations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rInYupXV2DH"
      },
      "source": [
        "### Testing images with different $\\omega_0$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlfE-zX6WFZx"
      },
      "source": [
        "iterations= 500\n",
        "hidden_layers = 2\n",
        "\n",
        "# 91 is the value of omega_0* (discussed in the report) for the image under analysis \n",
        "omega_values = [1, 30, 91, 1000, 2000] \n",
        "image_name = \"0803\"\n",
        "results = []\n",
        "for omega_0 in omega_values:\n",
        "  # Load image\n",
        "  image = data_utils.get_div2k_image(image_name, dir=BASE_DIR+\"/\", resolution='low')\n",
        "  dataset = ImageFitting(data_image=image, name=image_name, normalized=True)\n",
        "\n",
        "  # Prepare the trainer\n",
        "  dims = NetworkDimensions(2, dataset.channels, hidden_layers=hidden_layers, first_omega_0 = omega_0)\n",
        "  trainer = Trainer(NetworkParams(description=f\"siren_w{omega_0}\"), device=DEVICE)\n",
        "  trainer.train(dims, [dataset], [iterations], summary_fn=summary_utils.image_fitting_summary, \n",
        "                loss_fn=data_utils.get_mse, lr=5e-4,\n",
        "                patience = max(100, 10*hidden_layers),\n",
        "                with_log=False)\n",
        "  trainer.test(dataset, validate_fn=summary_utils.image_fitting_validate)\n",
        "  \n",
        "  # Load and save result\n",
        "  mean = trainer.statistics(compute_PSNR=True)[0][0]\n",
        "  results.append(mean)\n",
        "  print(f\"PSNR Mean {mean} dB for omega_0 = {omega_0}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXjaJ5AcKmKg"
      },
      "source": [
        "# Plot results\n",
        "fig = plt.figure()\n",
        "ax = fig.add_axes([0,0,1,1])\n",
        "ax.bar(['1', '30', '$\\omega_0^*$','1000', '2000'],results)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzHNCzN9dGVD"
      },
      "source": [
        "## First layer initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUnY0rb_eKyU"
      },
      "source": [
        "#### He initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOkrvwEpdPyp"
      },
      "source": [
        "# initialization function definition\r\n",
        "init = lambda weights : nn.init.kaiming_normal_(weights, a=0.0, nonlinearity='relu', mode='fan_in')\r\n",
        "\r\n",
        "dims = NetworkDimensions(in_features=1, out_features=1, hidden_layers=2, hidden_features=2048, first_omega_0=30)\r\n",
        "params = NetworkParams(outermost_linear=True, first_init=init)\r\n",
        "model = Network(params=params, dimensions=dims)\r\n",
        "\r\n",
        "input_signal = torch.linspace(-1, 1, 65536//4).view(1, -1, 1)\r\n",
        "# generating the output and activations for a uniform input\r\n",
        "activations = model.forward_with_activations(input_signal, retain_grad=True)\r\n",
        "output = activations[next(reversed(activations))]\r\n",
        "output.mean().backward()\r\n",
        "\r\n",
        "# plot activations at every layer\r\n",
        "data_utils.plot_all_activations_and_grads(activations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vK0jfFhKeX8j"
      },
      "source": [
        "#### Xavier initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TuH5gfTeYhf"
      },
      "source": [
        "# initialization function definition\r\n",
        "init =  lambda weights : nn.init.xavier_uniform_(weights)\r\n",
        "\r\n",
        "dims = NetworkDimensions(in_features=1, out_features=1, hidden_layers=2, hidden_features=2048, first_omega_0=30)\r\n",
        "params = NetworkParams(outermost_linear=True, first_init=init)\r\n",
        "model = Network(params=params, dimensions=dims)\r\n",
        "\r\n",
        "input_signal = torch.linspace(-1, 1, 65536//4).view(1, -1, 1)\r\n",
        "# generating the output and activations for a uniform input\r\n",
        "activations = model.forward_with_activations(input_signal, retain_grad=True)\r\n",
        "output = activations[next(reversed(activations))]\r\n",
        "output.mean().backward()\r\n",
        "\r\n",
        "# plot activations at every layer\r\n",
        "data_utils.plot_all_activations_and_grads(activations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rTBJ40Cff8O"
      },
      "source": [
        "### Testing images with different initializations of first layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jntcAKmpfpje"
      },
      "source": [
        "iterations= 500\r\n",
        "hidden_layers = 2\r\n",
        "image_name = \"0803\"\r\n",
        "inits = [None, lambda weights : nn.init.kaiming_normal_(weights, a=0.0, nonlinearity='relu', mode='fan_in'), lambda weights : nn.init.xavier_uniform_(weights) ]\r\n",
        "descriptions = [\"Sitzmann\", \"He\", \"Xavier\"]\r\n",
        "\r\n",
        "for init,description in zip(inits, descriptions):\r\n",
        "  # training the network on an image for every initialization scheme\r\n",
        "  trainer = Trainer(NetworkParams(description=f\"first_init_{description}\", first_init=init), device=DEVICE)\r\n",
        "  image = data_utils.get_div2k_image(image_name, dir=BASE_DIR+\"/\", resolution='low')\r\n",
        "  dataset = ImageFitting(data_image=image, name= image_name, normalized=True)\r\n",
        "  \r\n",
        "  dims = NetworkDimensions(2, dataset.channels, hidden_layers=hidden_layers, first_omega_0 = 96)\r\n",
        "\r\n",
        "  trainer.train(dims, [dataset], [iterations], summary_fn=summary_utils.image_fitting_summary, \r\n",
        "                loss_fn=data_utils.get_mse, lr=5e-4,\r\n",
        "                patience = max(100, 10*hidden_layers))\r\n",
        "  trainer.test(dataset, validate_fn=summary_utils.image_fitting_validate)\r\n",
        "\r\n",
        "\r\n",
        "  mean, std = trainer.statistics(compute_PSNR=True)\r\n",
        "  print(f\"PSNR Mean {mean[0]} dB and std {std[0]} dB for first init = {description}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPb7x-guh_wX"
      },
      "source": [
        "## Hidden layers initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Xmo2wlJh_wY"
      },
      "source": [
        "#### He initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tgqjZVah_wY"
      },
      "source": [
        " # initialization function definition\r\n",
        "init = lambda weights : nn.init.kaiming_normal_(weights, a=0.0, nonlinearity='relu', mode='fan_in')\r\n",
        "\r\n",
        "dims = NetworkDimensions(in_features=1, out_features=1, hidden_layers=2, hidden_features=2048, first_omega_0=30)\r\n",
        "params = NetworkParams(outermost_linear=True, hidden_init=init)\r\n",
        "model = Network(params=params, dimensions=dims)\r\n",
        "\r\n",
        "input_signal = torch.linspace(-1, 1, 65536//4).view(1, -1, 1)\r\n",
        "# generating the output and activations for a uniform input\r\n",
        "activations = model.forward_with_activations(input_signal, retain_grad=True)\r\n",
        "output = activations[next(reversed(activations))]\r\n",
        "output.mean().backward()\r\n",
        "\r\n",
        "# plot activations at every layer\r\n",
        "data_utils.plot_all_activations_and_grads(activations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cJ3a8dsh_wZ"
      },
      "source": [
        "#### Xavier initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLIwqFB_h_wZ"
      },
      "source": [
        "# initialization function definition\r\n",
        "init =  lambda weights : nn.init.xavier_uniform_(weights)\r\n",
        "\r\n",
        "dims = NetworkDimensions(in_features=1, out_features=1, hidden_layers=2, hidden_features=2048, first_omega_0=30)\r\n",
        "params = NetworkParams(outermost_linear=True, hidden_init=init)\r\n",
        "model = Network(params=params, dimensions=dims)\r\n",
        "\r\n",
        "input_signal = torch.linspace(-1, 1, 65536//4).view(1, -1, 1)\r\n",
        "# generating the output and activations for a uniform input\r\n",
        "activations = model.forward_with_activations(input_signal, retain_grad=True)\r\n",
        "output = activations[next(reversed(activations))]\r\n",
        "output.mean().backward()\r\n",
        "\r\n",
        "# plot activations at every layer\r\n",
        "data_utils.plot_all_activations_and_grads(activations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcVoYf_sjF3A"
      },
      "source": [
        "### Testing images with different initializations of hidden layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_ymMlbfjF3A"
      },
      "source": [
        "iterations= 500\r\n",
        "hidden_layers = 2\r\n",
        "image_name = \"0803\"\r\n",
        "inits = [None, lambda weights : nn.init.kaiming_normal_(weights, a=0.0, nonlinearity='relu', mode='fan_in'), lambda weights : nn.init.xavier_uniform_(weights) ]\r\n",
        "descriptions = [\"Sitzmann\", \"He\", \"Xavier\"]\r\n",
        "\r\n",
        "for init,description in zip(inits, descriptions):\r\n",
        "  # training the network on an image for every initialization scheme\r\n",
        "  trainer = Trainer(NetworkParams(description=f\"hidden_init_{description}\", hidden_init=init), device=DEVICE)\r\n",
        "  image = data_utils.get_div2k_image(image_name, dir=BASE_DIR+\"/\", resolution='low')\r\n",
        "  dataset = ImageFitting(data_image=image, name= image_name, normalized=True)\r\n",
        "  \r\n",
        "  dims = NetworkDimensions(2, dataset.channels, hidden_layers=hidden_layers, first_omega_0 = 30)\r\n",
        "\r\n",
        "  trainer.train(dims, [dataset], [iterations], summary_fn=summary_utils.image_fitting_summary, \r\n",
        "                loss_fn=data_utils.get_mse, lr=1e-4,\r\n",
        "                patience = max(100, 10*hidden_layers))\r\n",
        "  trainer.test(dataset, validate_fn=summary_utils.image_fitting_validate)\r\n",
        "\r\n",
        "\r\n",
        "  mean, std = trainer.statistics(compute_PSNR=True)\r\n",
        "  print(f\"PSNR Mean {mean[0]} dB and std {std[0]} dB for hidden init = {description}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}